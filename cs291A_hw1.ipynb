{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a-nawel/hw1/blob/main/cs291A_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vlutbqe3GKua"
      },
      "source": [
        "# data_utils.py "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1Mxs3GFGNfW"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class NormalizeByChannelMeanStd(torch.nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(NormalizeByChannelMeanStd, self).__init__()\n",
        "        if not isinstance(mean, torch.Tensor):\n",
        "            mean = torch.tensor(mean)\n",
        "        if not isinstance(std, torch.Tensor):\n",
        "            std = torch.tensor(std)\n",
        "        self.register_buffer(\"mean\", mean)\n",
        "        self.register_buffer(\"std\", std)\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        return self.normalize_fn(tensor, self.mean, self.std)\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'mean={}, std={}'.format(self.mean, self.std)\n",
        "\n",
        "    def normalize_fn(self, tensor, mean, std):\n",
        "        \"\"\"Differentiable version of torchvision.functional.normalize\"\"\"\n",
        "        # here we assume the color channel is in at dim=1\n",
        "        mean = mean[None, :, None, None]\n",
        "        std = std[None, :, None, None]\n",
        "        return tensor.sub(mean).div(std)\n",
        "\n",
        "def cifar10_dataloader(batch_size=64, data_dir='./data/', val_ratio=0.1):\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    train_size = int(50000 * (1 - val_ratio))\n",
        "    val_size = 50000 - train_size\n",
        "\n",
        "    train_set = Subset(CIFAR10(data_dir, train=True, transform=train_transform, download=True), list(range(train_size)))\n",
        "    val_set = Subset(CIFAR10(data_dir, train=True, transform=test_transform, download=True),\n",
        "                     list(range(train_size, train_size + val_size)))\n",
        "    test_set = CIFAR10(data_dir, train=False, transform=test_transform, download=True)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    dataset_normalization = NormalizeByChannelMeanStd(\n",
        "        mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
        "    return train_loader, val_loader, test_loader, dataset_normalization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KA4T0sUTMYI"
      },
      "source": [
        "# model_util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUmUpd-NCp0s"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicModule, self).__init__()\n",
        "        self.model_name = str(type(self))\n",
        "\n",
        "    def load(self, path, map_location=None):\n",
        "        self.load_state_dict(torch.load(path, map_location))\n",
        "\n",
        "    def save(self, name=None):\n",
        "        if name is None:\n",
        "            prefix = 'checkpoints/' + self.model_name + '_'\n",
        "            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')\n",
        "        torch.save(self.state_dict(), name)\n",
        "        return name\n",
        "\n",
        "    def no_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def with_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def clear_grad(self):\n",
        "        for param in self.parameters():\n",
        "            param.grad = None\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, activation_fn=nn.ReLU()):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion * planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.activation_fn(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = self.activation_fn(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(BasicModule):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, activation_fn=nn.ReLU, conv1_size=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.activation_fn = activation_fn(beta=10) if activation_fn == nn.Softplus else activation_fn()\n",
        "\n",
        "        kernel_size, stride, padding = {3: [3, 1, 1], 7: [7, 2, 3], 15: [15, 3, 7]}[conv1_size]\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, activation_fn=self.activation_fn)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, activation_fn=self.activation_fn)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, activation_fn=self.activation_fn)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, activation_fn=self.activation_fn)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        self.normalize = None\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, activation_fn=nn.ReLU()):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, activation_fn))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, penu=False):\n",
        "        if not self.normalize:\n",
        "            x = self.normalize(x)\n",
        "        out = self.activation_fn(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        if penu:\n",
        "            return out\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18(num_classes=10, conv1_size=3, activation_fn=nn.ReLU):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, conv1_size=conv1_size, activation_fn=activation_fn)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3mYA5urGvYB"
      },
      "source": [
        "# attack_util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0E2OReUTlhc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "### Do not modif the following codes\n",
        "class ctx_noparamgrad(object):\n",
        "    def __init__(self, module):\n",
        "        self.prev_grad_state = get_param_grad_state(module)\n",
        "        self.module = module\n",
        "        set_param_grad_off(module)\n",
        "\n",
        "    def __enter__(self):\n",
        "        pass\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        set_param_grad_state(self.module, self.prev_grad_state)\n",
        "        return False\n",
        "        \n",
        "def get_param_grad_state(module):\n",
        "    return {param: param.requires_grad for param in module.parameters()}\n",
        "\n",
        "def set_param_grad_off(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "def set_param_grad_state(module, grad_state):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad = grad_state[param]\n",
        "\n",
        "### Ends\n",
        "\n",
        "\n",
        "### PGD Attack\n",
        "class PGDAttack():\n",
        "    def __init__(self, step = 10, eps = 8 / 255, alpha = 0.01, loss_type = 'ce', targeted = True, \n",
        "                    num_classes = 10, norm = 'linf', fgsm = False):\n",
        "        '''\n",
        "        norm: this parameter means which type of l-p norm constraints we use for attack. Note that we onlyuse L-inf norm for our homework. \n",
        "        Therefore, this parameter is always linf.\n",
        "        But if you are interested in implementing an l-2 norm bounded attack, you can also try to implement it. Note that in that case,\n",
        "        the eps should be set to a larger value such as 200/255 because of the difference between l-2 and l-inf.\n",
        "        '''\n",
        "        self.attack_step = step\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.loss_type = loss_type\n",
        "        self.targeted = targeted\n",
        "        self.num_classes = num_classes\n",
        "        self.norm = norm\n",
        "        self.fgsm = fgsm\n",
        "\n",
        "\n",
        "    def ce_loss(self, logits, ys, reduction = 'none'):\n",
        "        ### Your code here\n",
        "        return nn.CrossEntropyLoss()(logits, ys)\n",
        "        ### Your code ends\n",
        "\n",
        "    def cw_loss(self, logits, ys, reduction = 'none'):\n",
        "        ### Your code here\n",
        "        if self.targeted:\n",
        "            #torch.max\n",
        "            #return max(-, 0)\n",
        "            \n",
        "\n",
        "\n",
        "            pass\n",
        "        else:\n",
        "            pass\n",
        "        ### Your code ends\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def clamp(self, delta, lower, upper):\n",
        "        ### Your code here\n",
        "\n",
        "        ### Your code ends\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def linf_proj(self, delta):\n",
        "        ### Your code here\n",
        "        return torch.clamp(delta, -self.eps, self.eps)\n",
        "        ### Your code ends\n",
        "\n",
        "    def perturb(self, model: nn.Module, Xs, ys):\n",
        "        delta = torch.zeros_like(Xs).to(Xs).uniform_(-self.eps, self.eps)\n",
        "        delta.requires_grad = True\n",
        "        \n",
        "        for iter_idx in range(self.attack_step):\n",
        "            ### Your code here\n",
        "            loss = self.ce_loss(model(Xs + delta), ys)\n",
        "            # Calculate the gradients\n",
        "            loss.backward()\n",
        "            delta.data = self.linf_proj(delta + self.alpha*delta.grad.detach().sign())\n",
        "            #delta.data = self.linf_proj(delta + Xs.shape[0]*self.alpha*delta.grad.data)\n",
        "            delta.grad.zero_()\n",
        "        return delta.detach() \n",
        "        ### Your code ends  \n",
        "\n",
        "\n",
        "\n",
        "### FGSMAttack\n",
        "'''\n",
        "Theoretically you can transform your PGDAttack to FGSM Attack by controling some of its parameters like `attack_step`. \n",
        "If you do that, you do not need to implement FGSM in this class.\n",
        "'''\n",
        "class FGSMAttack():\n",
        "    def __init__(self, eps = 8 / 255, loss_type = 'ce', targeted = True, num_classes = 10, norm = 'linf'):\n",
        "        self.eps = eps\n",
        "        self.loss_type = loss_type\n",
        "        self.targeted = targeted\n",
        "        self.num_classes = num_classes\n",
        "        self.norm = norm\n",
        "\n",
        "    def perturb(self, model: nn.Module, Xs, ys):\n",
        "        delta = torch.zeros_like(Xs).to(Xs)\n",
        "        delta.requires_grad = True\n",
        "        ### Your code here\n",
        "        #loss = self.ce_loss(model(Xs + delta), ys)\n",
        "        # Calculate the gradients\n",
        "        #loss.backward()\n",
        "\n",
        "        \n",
        "        #delta = self.eps * delta.grad.detach().sign()\n",
        "        ### Your code ends\n",
        "        pass\n",
        "        return delta        \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU298oMOG9E3"
      },
      "source": [
        "# evaluate.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDyMD-ZzG_Ir",
        "outputId": "e4e63b12-4da0-42a3-8f61-621df3ac93d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [02:57<00:00,  1.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 10000, correct 9483, adversarial correct 231, clean accuracy 0.9483, robust accuracy 0.0231\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#import data_util\n",
        "#import model_util\n",
        "#import attack_util\n",
        "#from attack_util import ctx_noparamgrad\n",
        "\n",
        "from tqdm import tqdm\n",
        "from io import BytesIO\n",
        "\n",
        "eps = 8\n",
        "alpha = 8\n",
        "attack_rs = 1\n",
        "attack_step = 10\n",
        "loss_type = 'ce'\n",
        "data_dir ='./data/'\n",
        "model_prefix = './checkpoints/'\n",
        "              \n",
        "model_name ='resnet_cifar10.pth'\n",
        "fgsm = False\n",
        "targeted = False\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(0) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "train_loader, valid_loader, test_loader, norm_layer = cifar10_dataloader(data_dir = data_dir)\n",
        "num_classes = 10\n",
        "model = ResNet18(num_classes = num_classes)\n",
        "model.normalize = norm_layer\n",
        "model_path = model_prefix + model_name\n",
        "\n",
        "with open(model_path, 'rb') as fh:\n",
        "    buf = BytesIO(fh.read())\n",
        "\n",
        "model.load(buf)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "#attack_step = attack_step\n",
        "eps = eps / 255\n",
        "restart = attack_rs\n",
        "alpha = alpha / 255\n",
        "#fgsm = fgsm\n",
        "#loss_type = loss_type\n",
        "#targeted = targeted\n",
        "\n",
        "### Your code here for creating the attacker object\n",
        "if fgsm:\n",
        "    attacker = FGSMAttack(eps = eps, loss_type = loss_type, targeted = targeted, num_classes = num_classes, norm = 'linf')\n",
        "else:\n",
        "    attacker = PGDAttack(step = attack_step, eps = eps, alpha = alpha, \n",
        "                                    loss_type = loss_type, targeted = targeted, num_classes = num_classes, \n",
        "                                    norm = 'linf', fgsm = fgsm)\n",
        "\n",
        "### Your code ends\n",
        "\n",
        "total = 0\n",
        "clean_correct_num = 0\n",
        "robust_correct_num = 0\n",
        "target_label = 1 ## only for targeted attack\n",
        "\n",
        "\n",
        "## Make sure the model is in `eval` mode. Otherwise some operations such as dropout will  \n",
        "model.eval()\n",
        "\n",
        "for data, labels in tqdm(test_loader):\n",
        "    data = data.float().to(device)\n",
        "    if targeted:\n",
        "        data_mask = (labels != target_label)\n",
        "        if data_mask.sum() == 0:\n",
        "            continue\n",
        "        data = data[data_mask]\n",
        "        labels = labels[data_mask]\n",
        "        attack_labels = torch.ones_like(labels).to(device)\n",
        "    else:\n",
        "        attack_labels = labels\n",
        "    attack_labels = attack_labels.to(device)\n",
        "    labels = labels.to(device) # added from email\n",
        "    batch_size = data.size(0)\n",
        "    total += batch_size\n",
        "    with ctx_noparamgrad(model):\n",
        "        ### generate perturbation\n",
        "        perturbed_data = attacker.perturb(model, data, attack_labels) + data\n",
        "\n",
        "        predictions = model(data)\n",
        "        clean_correct_num += torch.sum(torch.argmax(predictions, dim = -1) == labels).item()\n",
        "\n",
        "        predictions = model(perturbed_data)\n",
        "        robust_correct_num += torch.sum(torch.argmax(predictions, dim = -1) == labels).item()\n",
        "\n",
        "print(f\"total {total}, correct {clean_correct_num}, adversarial correct {robust_correct_num}, clean accuracy {clean_correct_num / total}, robust accuracy {robust_correct_num / total}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "cs291A-hw1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOcUh/mm8nQLvw/C/VaYbRr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}